{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract Extractor from Pubmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was designed to autopopulate a MS SQL database with a fixed structure. Instructions and SQL code for creating this database can be found on the Wiki page for this project: https://github.com/KimSebek/Public-Health-Research-Information-Tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PubMed_Searcher:\n",
    "    def __init__(self, email, retmode = 'xml'):\n",
    "        self.email = email\n",
    "        self.retmode = retmode\n",
    "        Entrez.email = email\n",
    "        \n",
    "    #get count of how many records are returned\n",
    "    def get_count(self, term):\n",
    "        counthandle = Entrez.egquery(term = term)\n",
    "        record = Entrez.read(counthandle)\n",
    "        for row in record[\"eGQueryResult\"]:\n",
    "            if row['DbName'] == 'pubmed':\n",
    "                rowcount = row['Count']\n",
    "        counthandle.close()\n",
    "        return rowcount\n",
    "        \n",
    "    #Used to interact with pubmed database\n",
    "    def fetch_details(self, id_list):\n",
    "        ids = ','.join(id_list)\n",
    "        \n",
    "        submitinterv = math.ceil(len(id_list)/10000)\n",
    "        \n",
    "        sleeptime = 5\n",
    "        \n",
    "        restartc = 0\n",
    "        \n",
    "        article_details = []\n",
    "        \n",
    "        for i in range(submitinterv):\n",
    "            dethandle = Entrez.efetch(db='pubmed',\n",
    "                                      retmode='xml',\n",
    "                                      id=ids,\n",
    "                                      retstart = restartc,\n",
    "                                      retmax=10000)\n",
    "            detailresults = Entrez.read(dethandle)\n",
    "            article_details += detailresults['PubmedArticle']\n",
    "            time.sleep(sleeptime)\n",
    "            restartc += 10000\n",
    "            dethandle.close()\n",
    "\n",
    "        return article_details\n",
    "    \n",
    "    #function for retrieving and storing ids\n",
    "    def pub_search(self,terms,sort='relevance',chunksize = 500,sleeptime = 5):\n",
    "        \n",
    "        submitinterv = math.ceil(int(self.get_count(term = terms))/chunksize)\n",
    "        \n",
    "        #append to idlist\n",
    "        idlist = []\n",
    "\n",
    "        retstartc = 0\n",
    "        for i in range(submitinterv):\n",
    "            handle = Entrez.esearch(db='pubmed', \n",
    "                                sort='relevance', \n",
    "                                retstart= retstartc,\n",
    "                                retmax=chunksize,\n",
    "                                retmode='xml', \n",
    "                                term=terms)\n",
    "            idresults = Entrez.read(handle)\n",
    "            idlist += idresults['IdList']\n",
    "            time.sleep(sleeptime)\n",
    "            retstartc += chunksize\n",
    "            handle.close()\n",
    "                \n",
    "        return idlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_df(articles):\n",
    "    article_list = []\n",
    "    for article in articles:\n",
    "        \n",
    "        ####### title ########\n",
    "        title = article['MedlineCitation']['Article']['ArticleTitle']\n",
    "        \n",
    "        ####### pmid #########\n",
    "        pmid = article['MedlineCitation']['PMID'].title()\n",
    "        \n",
    "        ####### journal ########\n",
    "        journal = article['MedlineCitation']['Article']['Journal']['Title']\n",
    "        \n",
    "        \n",
    "        ####### meshterms ##########\n",
    "        \n",
    "        #each meshterm will have its word and possibly some qualifiernames\n",
    "        meshterm_list = []\n",
    "        try:\n",
    "            for meshterm in article['MedlineCitation']['MeshHeadingList']:\n",
    "                #reset qualifiernames_list\n",
    "                qualifiernames_list = []\n",
    "                \n",
    "                term = meshterm['DescriptorName'].title()\n",
    "                if(len(meshterm['QualifierName']) != 0):\n",
    "                    qualifiernames_list = [qn.title() for qn in meshterm['QualifierName']]\n",
    "                \n",
    "                meshterm_dict = {\n",
    "                    'term' : term,\n",
    "                    'qualifiernames' : qualifiernames_list\n",
    "                }\n",
    "                \n",
    "                meshterm_list.append(meshterm_dict)\n",
    "        except:\n",
    "            meshterm_list = None\n",
    "            \n",
    "               \n",
    "        ######## article date ###########\n",
    "            \n",
    "        if article['MedlineCitation']['Article']['ArticleDate']:\n",
    "            year = article['MedlineCitation']['Article']['ArticleDate'][0]['Year']\n",
    "            month = article['MedlineCitation']['Article']['ArticleDate'][0]['Month']\n",
    "            day = article['MedlineCitation']['Article']['ArticleDate'][0]['Day']\n",
    "            fulldate = '{0}/{1}/{2}'.format(month,day,year)\n",
    "        if not article['MedlineCitation']['Article']['ArticleDate']:\n",
    "            #sometimes the there is no DateCompleted either, so we will use a date given from the article history\n",
    "            try:\n",
    "                year = article['MedlineCitation']['DateCompleted']['Year']\n",
    "                month = article['MedlineCitation']['DateCompleted']['Month']\n",
    "                day = article['MedlineCitation']['DateCompleted']['Day']\n",
    "                fulldate = '{0}/{1}/{2}'.format(month,day,year)\n",
    "            except:            \n",
    "                year = article['PubmedData']['History'][0]['Year']\n",
    "                month = article['PubmedData']['History'][0]['Month']\n",
    "                day = article['PubmedData']['History'][0]['Day']\n",
    "                fulldate = '{0}/{1}/{2}'.format(month,day,year)\n",
    "        \n",
    "        \n",
    "        ######### abstract ##########\n",
    "         \n",
    "        #each article's abstract will be stored as a dictionary. Each key being a different section of the abstract\n",
    "        abstract_list = []\n",
    "        try:\n",
    "            try:\n",
    "                #this means abstract is broken up by nlmcategory\n",
    "                for abstractpart in article['MedlineCitation']['Article']['Abstract']['AbstractText']:\n",
    "                    abstract_dict = {}\n",
    "                    nlmcategory = abstractpart.attributes['NlmCategory'].lower()\n",
    "                    label = abstractpart.attributes['Label'].lower()\n",
    "                    text = abstractpart\n",
    "                    abstract_dict['Label'] = label\n",
    "                    abstract_dict['NlmCategory'] = nlmcategory\n",
    "                    abstract_dict['text'] = text\n",
    "                    \n",
    "                    abstract_list.append(abstract_dict)\n",
    "            except:\n",
    "                #this means the abstract comes in as one whole chunk\n",
    "                abstract_dict = {}\n",
    "                wholetext = article['MedlineCitation']['Article']['Abstract']['AbstractText'][0]\n",
    "                abstract_dict['wholetext'] = wholetext\n",
    "                \n",
    "                abstract_list.append(abstract_dict)\n",
    "        except:\n",
    "            #this means there is no abstract at all\n",
    "            abstract_list = None\n",
    "            \n",
    "            \n",
    "        ############ authors ###############\n",
    "        \n",
    "        #each article will have a list of authors. Each author is represented by a dictionary (key values: fname, lname, affliation)\n",
    "        auth_list = []\n",
    "        #some articles don't have an author\n",
    "        try:          \n",
    "            #loop through authorlist for each article\n",
    "            for author in article['MedlineCitation']['Article']['AuthorList']:\n",
    "                \n",
    "                #certain articles just have a collective name, for ex: CDC\n",
    "                #those with a collective name don't have a forename or lastname, thus they are set to None\n",
    "                try:\n",
    "                    affiliation = author['CollectiveName']\n",
    "                    lname = None\n",
    "                    fname = None\n",
    "                except:\n",
    "                    try:\n",
    "                        lname = author['LastName']\n",
    "                    except:\n",
    "                        lname = None\n",
    "\n",
    "                    try:\n",
    "                        fname = author['ForeName']\n",
    "                    except:\n",
    "                        fname = None\n",
    "\n",
    "                    try:\n",
    "                        affiliation = author['AffiliationInfo'][0]['Affiliation']\n",
    "                    except:\n",
    "                        affiliation = None\n",
    "                \n",
    "                auth_dict = {\n",
    "                    'lname' : lname,\n",
    "                    'fname' : fname,\n",
    "                    'affiliation' : affiliation\n",
    "                }\n",
    "                \n",
    "                auth_list.append(auth_dict)\n",
    "        except:\n",
    "            auth_list = None\n",
    "            \n",
    "            \n",
    "        ######### optionalid01 ##########\n",
    "        \n",
    "        optionalid01 = None\n",
    "        \n",
    "        for elocationid in article['MedlineCitation']['Article']['ELocationID']:\n",
    "            if elocationid.attributes['EIdType'] == 'doi':\n",
    "                optionalid01 = elocationid    \n",
    "                \n",
    "            \n",
    "        d = {\n",
    "            'title': title,\n",
    "            'pmid' : pmid,\n",
    "            'author' : auth_list,\n",
    "            'journal' : journal,\n",
    "            'publishdate' : fulldate,\n",
    "            'meshterms' : meshterm_list,\n",
    "            'abstract' : abstract_list,\n",
    "            'optionalid01' : optionalid01\n",
    "        }\n",
    "    \n",
    "        article_list.append(d)\n",
    "            \n",
    "    return pd.DataFrame(article_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### query term #########\n",
    "term = r'term'#insert your term here - this works best for a broad, generic keyword such as \"diabetes\"\n",
    "\n",
    "####### email for pubmed #######\n",
    "email = r'user@email.com'#insert your email here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pubmed_obj = PubMed_Searcher(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmidlist = Pubmed_obj.pub_search(terms=term,chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = Pubmed_obj.fetch_details(pmidlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = get_article_df(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41242"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# moving abstracts into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### establish connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server = r'SERVER' #insert your server name here\n",
    "database = r'DBASE' #insert your database name here\n",
    "\n",
    "#trusted connection enables use of windows authentication(no need to enter your windows username/password)\n",
    "cnxn = pyodbc.connect(driver='{SQL Server}',server=server,database=database,trusted_connection='yes')\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataPull_ID table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pulldate = date.today().strftime('%m/%d/%Y')\n",
    "pullname = 'term' #insert your term here\n",
    "pullquery = term\n",
    "pulltype = 'keyword' #keyword/author\n",
    "pullsource = 'Pubmed'.upper()\n",
    "pullby = 'YOURNAME' #insert your name here \n",
    "\n",
    "query = \"\"\" insert into DataPull_ID (PullDate, PullName, PullQuery, PullType, PullSource, PullBy) \n",
    "            values (?, ?, ?, ?, ?, ?) \"\"\"\n",
    "\n",
    "args = (pulldate, pullname, pullquery, pulltype, pullsource, pullby)\n",
    "\n",
    "cursor.execute(query, args)\n",
    "#must commit in order to see it on sql server, if not sql server database won't load correctly\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataPull_Detail table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gets PullID that was auto generated from query above\n",
    "# do not run this if you are using refresh\n",
    "pullid = cursor.execute(\"select @@IDENTITY\").fetchall()[0][0].__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all AssociatedIDs already in the database\n",
    "existing_ids = {associatedid[0] for associatedid in cursor.execute(\"select associatedid from DataPull_Detail\").fetchall()}\n",
    "\n",
    "note = None\n",
    "\n",
    "query = \"\"\" insert into DataPull_Detail (PullID, AssociatedID, ValueStore, Note) values (?,?,?,?)\"\"\"\n",
    "\n",
    "values_list = []\n",
    "\n",
    "# check how many associatedIDs there are\n",
    "for index,row in results_df.iterrows():\n",
    "    \n",
    "    associatedid = str(row['pmid'])\n",
    "    \n",
    "    # check if associatedid is already in the database\n",
    "    if associatedid in existing_ids:\n",
    "        valuestore = 'duplicate'\n",
    "        # drop the row that already exists\n",
    "        results_df.drop(index, inplace=True)\n",
    "    else:\n",
    "        valuestore = 'store'\n",
    "        print('store')\n",
    "    \n",
    "    values_list.append((pullid,associatedid,valuestore,note))\n",
    "\n",
    "cursor.executemany(query,values_list)\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataPull_Title table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\" insert into DataPull_Title (AssociatedID, Title, Journal, PublicationDate, OptionalID01, OptionalID02) values (?,?,?,?,?,?) \"\"\"\n",
    "\n",
    "values_list = []\n",
    "\n",
    "for index,row in results_df.iterrows():\n",
    "    associatedid = str(row['pmid'])\n",
    "    title = row['title']\n",
    "    journal = row['journal']\n",
    "    pubdate = row['publishdate']\n",
    "    id01 = row['optionalid01']\n",
    "    \n",
    "    values_list.append((associatedid, title, journal ,pubdate, id01, None))\n",
    "\n",
    "cursor.executemany(query, values_list)\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataPull_Keyword table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\" insert into DataPull_Keyword (AssociatedID, KeywordValue, Category1, Category2,\n",
    "                    Category3, Category4, Category5) values (?,?,?,?,?,?,?)\"\"\"\n",
    "\n",
    "values_list = []\n",
    "\n",
    "for index,row in results_df.iterrows():\n",
    "    associatedid = str(row['pmid'])\n",
    "    \n",
    "    if row['meshterms'] is not None:\n",
    "        \n",
    "        #each meshterm comes in as a dictionary (keys: qualifiernames & term)\n",
    "        #qualifiernames value = a list of qualifernames\n",
    "        #term value = the actual mesh term\n",
    "        for word in row['meshterms']:\n",
    "            keywordvalue = word['term']\n",
    "            \n",
    "            num_of_qualifiers = len(word['qualifiernames'])\n",
    "            \n",
    "            #some have more than 5 qualifier names, if that is the case, then make num_of_Nones to 0 \n",
    "            #needed to fill up ? marks with NULLS\n",
    "            \n",
    "            if num_of_qualifiers > 5:\n",
    "                num_of_Nones = 0\n",
    "                word['qualifiernames'] = word['qualifiernames'][:5]\n",
    "            else:\n",
    "                num_of_Nones = 5 - num_of_qualifiers\n",
    "                \n",
    "            #if there are qualifier names, the list should not be 0\n",
    "            if len(word['qualifiernames']) != 0:\n",
    "                values_list.append(([associatedid,keywordvalue] + word['qualifiernames'] + [None]*num_of_Nones)) \n",
    "            else:\n",
    "                values_list.append((associatedid, keywordvalue, None, None, None, None, None))\n",
    "\n",
    "cursor.executemany(query,values_list)\n",
    "cnxn.commit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataPull_Authors table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\" insert into DataPull_Author (AssociatedID, ForeName, LastName, Affiliation) values (?,?,?,?) \"\"\"\n",
    "\n",
    "values_list = []\n",
    "\n",
    "for index,row in results_df.iterrows():\n",
    "    associatedid = str(row['pmid'])\n",
    "    auth_count = 0\n",
    "    \n",
    "    if row['author'] is not None:\n",
    "        for auth in row['author']:\n",
    "            auth_count += 1\n",
    "            if auth_count < 4:\n",
    "                values_list.append((associatedid, auth['fname'], auth['lname'], auth['affiliation']))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "cursor.executemany(query,values_list)\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataPull_Text table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\" insert into DataPull_text (associatedid, nlmcategory, label, abstracttext) values (?,?,?,?)\"\"\"\n",
    "\n",
    "values_list = []\n",
    "\n",
    "for index,row in results_df.iterrows():\n",
    "    associatedid = str(row['pmid'])\n",
    "    \n",
    "    if row['abstract'] is not None:\n",
    "        for part in row['abstract']:\n",
    "            \n",
    "            #check if the abstract is just one chunk\n",
    "            try:\n",
    "                values_list.append((associatedid, None, None, part['wholetext']))\n",
    "            except:\n",
    "                values_list.append((associatedid, part['NlmCategory'], part['Label'], part['text']))\n",
    "\n",
    "cursor.executemany(query,values_list)\n",
    "cnxn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
